{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASET_DIR = './data/'\n",
    "GLOVE_DIR = './glove.6B/'\n",
    "SAVE_DIR = './'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "val_x = pd.read_csv(os.path.join(DATASET_DIR, '4_feature_data/valid_set_7_8_12_15.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "val_x = val_x.drop(columns=[\"domain1_predictionid\", \"domain2_predictionid\"])\n",
    "\n",
    "# for some reason normed data works \"Terribly\"\n",
    "normed_x = pd.read_csv(os.path.join(DATASET_DIR, '4_feature_data/7_8_normalized_shifted.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "normed_x = normed_x.dropna(axis=1)\n",
    "normed_x = normed_x.drop(columns=['domain1_score','rater1_domain1','rater2_domain1','rater1_trait1', 'rater1_trait2', 'rater1_trait3', \n",
    "                            'rater1_trait4', 'rater2_trait1','rater2_trait2', 'rater2_trait3','rater2_trait4'])\n",
    "normed_y = normed_x[['f1','f2','f3','f4']]\n",
    "normed_y = normed_x[['7.1=8.1','7.2=8.2','7.3=8.4,8.5','7.4=8.6']]\n",
    "\n",
    "new_x = pd.read_csv(os.path.join(DATASET_DIR, '4_feature_data/training_7_8_full_features_1212.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "new_x = new_x.dropna(axis=1)\n",
    "new_x = new_x.drop(columns=['domain1_score','rater1_domain1','rater2_domain1','rater1_trait1', 'rater1_trait2', 'rater1_trait3', \n",
    "                            'rater1_trait4', 'rater2_trait1','rater2_trait2', 'rater2_trait3','rater2_trait4'])\n",
    "new_y = new_x[['7.1=8.1','7.2=8.2','7.3=8.4,8.5','7.4=8.6']]\n",
    "new_y.head()\n",
    "\n",
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "for_kappa = X[['rater1_domain1', 'rater2_domain1']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['essay_id', 'essay_set', 'essay', 'rater1_domain1', 'rater2_domain1',\n",
       "       'rater3_domain1', 'domain1_score', 'rater1_domain2', 'rater2_domain2',\n",
       "       'domain2_score', 'rater1_trait1', 'rater1_trait2', 'rater1_trait3',\n",
       "       'rater1_trait4', 'rater1_trait5', 'rater1_trait6', 'rater2_trait1',\n",
       "       'rater2_trait2', 'rater2_trait3', 'rater2_trait4', 'rater2_trait5',\n",
       "       'rater2_trait6', 'rater3_trait1', 'rater3_trait2', 'rater3_trait3',\n",
       "       'rater3_trait4', 'rater3_trait5', 'rater3_trait6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X[X.essay_set > 6]\n",
    "# X = X.dropna(axis=1)\n",
    "X.head()\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum and Maximum Scores for each essay set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess all essays and convert them to feature vectors so that they can be fed into the RNN.\n",
    "\n",
    "These are all helper functions used to clean the essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "#     index2word_set = set(model.wv.index2word)\n",
    "    index2word_set = set(model.wv.index_to_key) # new\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "#           featureVec = np.add(featureVec,model[word])\n",
    "            featureVec = np.add(featureVec,model.wv[word]) # new Aidan\n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a 2-Layer LSTM Model. \n",
    "\n",
    "Note that instead of using sigmoid activation in the output layer we will use\n",
    "Relu since we are not normalising training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/aidan_oneill/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/aidan_oneill/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/aidan_oneill/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/aidan_oneill/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/aidan_oneill/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/aidan_oneill/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/aidan_oneill/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/aidan_oneill/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/aidan_oneill/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/aidan_oneill/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/aidan_oneill/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/aidan_oneill/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4, activation='relu')) # this is important\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model on the dataset.\n",
    "\n",
    "We will use 5-Fold Cross Validation and measure the Quadratic Weighted Kappa for each fold.\n",
    "We will then calculate Average Kappa for all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 814,900\n",
      "Trainable params: 814,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/aidan_oneill/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/50\n",
      "1833/1833 [==============================] - 1s 737us/step - loss: 2.6378 - mae: 1.2690\n",
      "Epoch 2/50\n",
      "1833/1833 [==============================] - 0s 187us/step - loss: 1.0131 - mae: 0.7890\n",
      "Epoch 3/50\n",
      "1833/1833 [==============================] - 0s 192us/step - loss: 0.8968 - mae: 0.7429\n",
      "Epoch 4/50\n",
      "1833/1833 [==============================] - 0s 189us/step - loss: 0.8113 - mae: 0.7048\n",
      "Epoch 5/50\n",
      "1833/1833 [==============================] - 0s 222us/step - loss: 0.7569 - mae: 0.6790\n",
      "Epoch 6/50\n",
      "1833/1833 [==============================] - 0s 222us/step - loss: 0.7226 - mae: 0.6615\n",
      "Epoch 7/50\n",
      "1833/1833 [==============================] - 0s 204us/step - loss: 0.6674 - mae: 0.6414\n",
      "Epoch 8/50\n",
      "1833/1833 [==============================] - 0s 186us/step - loss: 0.6529 - mae: 0.6347\n",
      "Epoch 9/50\n",
      "1833/1833 [==============================] - 0s 189us/step - loss: 0.6520 - mae: 0.6291\n",
      "Epoch 10/50\n",
      "1833/1833 [==============================] - 0s 243us/step - loss: 0.6217 - mae: 0.6134\n",
      "Epoch 11/50\n",
      "1833/1833 [==============================] - 0s 193us/step - loss: 0.6257 - mae: 0.6180\n",
      "Epoch 12/50\n",
      "1833/1833 [==============================] - 0s 186us/step - loss: 0.6040 - mae: 0.6054\n",
      "Epoch 13/50\n",
      "1833/1833 [==============================] - 0s 190us/step - loss: 0.6219 - mae: 0.6169\n",
      "Epoch 14/50\n",
      "1833/1833 [==============================] - 0s 197us/step - loss: 0.5838 - mae: 0.5968\n",
      "Epoch 15/50\n",
      "1833/1833 [==============================] - 0s 191us/step - loss: 0.5779 - mae: 0.5966\n",
      "Epoch 16/50\n",
      "1833/1833 [==============================] - 0s 217us/step - loss: 0.5837 - mae: 0.5927\n",
      "Epoch 17/50\n",
      "1833/1833 [==============================] - 0s 184us/step - loss: 0.5792 - mae: 0.5952\n",
      "Epoch 18/50\n",
      "1833/1833 [==============================] - 0s 185us/step - loss: 0.5622 - mae: 0.5822\n",
      "Epoch 19/50\n",
      "1833/1833 [==============================] - 0s 188us/step - loss: 0.5471 - mae: 0.5796\n",
      "Epoch 20/50\n",
      "1833/1833 [==============================] - 0s 190us/step - loss: 0.5738 - mae: 0.5928\n",
      "Epoch 21/50\n",
      "1833/1833 [==============================] - 0s 190us/step - loss: 0.5148 - mae: 0.5656\n",
      "Epoch 22/50\n",
      "1833/1833 [==============================] - 0s 184us/step - loss: 0.5275 - mae: 0.5673\n",
      "Epoch 23/50\n",
      "1833/1833 [==============================] - 0s 183us/step - loss: 0.5053 - mae: 0.5566\n",
      "Epoch 24/50\n",
      "1833/1833 [==============================] - 0s 186us/step - loss: 0.5438 - mae: 0.5793\n",
      "Epoch 25/50\n",
      "1833/1833 [==============================] - 0s 193us/step - loss: 0.5100 - mae: 0.5591\n",
      "Epoch 26/50\n",
      "1833/1833 [==============================] - 0s 182us/step - loss: 0.5438 - mae: 0.5777\n",
      "Epoch 27/50\n",
      "1833/1833 [==============================] - 0s 187us/step - loss: 0.5109 - mae: 0.5610\n",
      "Epoch 28/50\n",
      "1833/1833 [==============================] - 0s 180us/step - loss: 0.5356 - mae: 0.5734\n",
      "Epoch 29/50\n",
      "1833/1833 [==============================] - 0s 181us/step - loss: 0.5100 - mae: 0.5566\n",
      "Epoch 30/50\n",
      "1833/1833 [==============================] - 0s 182us/step - loss: 0.4947 - mae: 0.5530\n",
      "Epoch 31/50\n",
      "1833/1833 [==============================] - 0s 186us/step - loss: 0.4961 - mae: 0.5448\n",
      "Epoch 32/50\n",
      "1833/1833 [==============================] - 0s 224us/step - loss: 0.5008 - mae: 0.5526\n",
      "Epoch 33/50\n",
      "1833/1833 [==============================] - 0s 186us/step - loss: 0.4811 - mae: 0.5434\n",
      "Epoch 34/50\n",
      "1833/1833 [==============================] - 0s 180us/step - loss: 0.4925 - mae: 0.5464\n",
      "Epoch 35/50\n",
      "1833/1833 [==============================] - 0s 178us/step - loss: 0.4990 - mae: 0.5526\n",
      "Epoch 36/50\n",
      "1833/1833 [==============================] - 0s 181us/step - loss: 0.4963 - mae: 0.5516\n",
      "Epoch 37/50\n",
      "1833/1833 [==============================] - 0s 178us/step - loss: 0.4864 - mae: 0.5471\n",
      "Epoch 38/50\n",
      "1833/1833 [==============================] - 0s 180us/step - loss: 0.4733 - mae: 0.5370\n",
      "Epoch 39/50\n",
      "1833/1833 [==============================] - 0s 179us/step - loss: 0.4799 - mae: 0.5418\n",
      "Epoch 40/50\n",
      "1833/1833 [==============================] - 0s 184us/step - loss: 0.4849 - mae: 0.5482\n",
      "Epoch 41/50\n",
      "1833/1833 [==============================] - 0s 179us/step - loss: 0.4629 - mae: 0.5351\n",
      "Epoch 42/50\n",
      "1833/1833 [==============================] - 0s 178us/step - loss: 0.4934 - mae: 0.5469\n",
      "Epoch 43/50\n",
      "1833/1833 [==============================] - 0s 215us/step - loss: 0.4921 - mae: 0.5478\n",
      "Epoch 44/50\n",
      "1833/1833 [==============================] - 0s 195us/step - loss: 0.4705 - mae: 0.5418\n",
      "Epoch 45/50\n",
      "1833/1833 [==============================] - 0s 182us/step - loss: 0.4882 - mae: 0.5460\n",
      "Epoch 46/50\n",
      "1833/1833 [==============================] - 0s 184us/step - loss: 0.4753 - mae: 0.5419\n",
      "Epoch 47/50\n",
      "1833/1833 [==============================] - 0s 181us/step - loss: 0.4626 - mae: 0.5329\n",
      "Epoch 48/50\n",
      "1833/1833 [==============================] - 0s 223us/step - loss: 0.4779 - mae: 0.5442\n",
      "Epoch 49/50\n",
      "1833/1833 [==============================] - 0s 191us/step - loss: 0.4519 - mae: 0.5286\n",
      "Epoch 50/50\n",
      "1833/1833 [==============================] - 0s 200us/step - loss: 0.4685 - mae: 0.5382\n",
      "Kappa Score for category 0: 0.8057154385673427\n",
      "Kappa Score for category 1: 0.7818013348194502\n",
      "Kappa Score for category 2: 0.792599658600261\n",
      "Kappa Score for category 3: 0.6573136081693599\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 814,900\n",
      "Trainable params: 814,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1833/1833 [==============================] - 1s 399us/step - loss: 2.4724 - mae: 1.2317\n",
      "Epoch 2/50\n",
      "1833/1833 [==============================] - 0s 184us/step - loss: 1.0371 - mae: 0.7954\n",
      "Epoch 3/50\n",
      "1833/1833 [==============================] - 0s 187us/step - loss: 0.8628 - mae: 0.7326\n",
      "Epoch 4/50\n",
      "1833/1833 [==============================] - 0s 191us/step - loss: 0.7865 - mae: 0.7062\n",
      "Epoch 5/50\n",
      "1833/1833 [==============================] - 0s 191us/step - loss: 0.7430 - mae: 0.6793\n",
      "Epoch 6/50\n",
      "1833/1833 [==============================] - 0s 183us/step - loss: 0.7016 - mae: 0.6598\n",
      "Epoch 7/50\n",
      "1833/1833 [==============================] - 0s 192us/step - loss: 0.6688 - mae: 0.6459\n",
      "Epoch 8/50\n",
      "1833/1833 [==============================] - 0s 185us/step - loss: 0.6703 - mae: 0.6453\n",
      "Epoch 9/50\n",
      "1833/1833 [==============================] - 0s 189us/step - loss: 0.6153 - mae: 0.6200\n",
      "Epoch 10/50\n",
      "1833/1833 [==============================] - 0s 184us/step - loss: 0.5944 - mae: 0.6067\n",
      "Epoch 11/50\n",
      "1833/1833 [==============================] - 0s 182us/step - loss: 0.5887 - mae: 0.5974\n",
      "Epoch 12/50\n",
      "1833/1833 [==============================] - 0s 182us/step - loss: 0.5888 - mae: 0.6029\n",
      "Epoch 13/50\n",
      "1833/1833 [==============================] - 0s 185us/step - loss: 0.5699 - mae: 0.5957\n",
      "Epoch 14/50\n",
      "1833/1833 [==============================] - 0s 180us/step - loss: 0.5563 - mae: 0.5874\n",
      "Epoch 15/50\n",
      "1833/1833 [==============================] - 0s 185us/step - loss: 0.5900 - mae: 0.6043\n",
      "Epoch 16/50\n",
      "1833/1833 [==============================] - 0s 184us/step - loss: 0.5577 - mae: 0.5878\n",
      "Epoch 17/50\n",
      "1833/1833 [==============================] - 0s 185us/step - loss: 0.5460 - mae: 0.5847\n",
      "Epoch 18/50\n",
      "1833/1833 [==============================] - 0s 181us/step - loss: 0.5451 - mae: 0.5831\n",
      "Epoch 19/50\n",
      "1833/1833 [==============================] - 0s 190us/step - loss: 0.5445 - mae: 0.5780\n",
      "Epoch 20/50\n",
      "1833/1833 [==============================] - 0s 180us/step - loss: 0.5245 - mae: 0.5670\n",
      "Epoch 21/50\n",
      "1833/1833 [==============================] - 0s 184us/step - loss: 0.5082 - mae: 0.5642\n",
      "Epoch 22/50\n",
      "1833/1833 [==============================] - 0s 183us/step - loss: 0.5484 - mae: 0.5823\n",
      "Epoch 23/50\n",
      "1833/1833 [==============================] - 0s 183us/step - loss: 0.5168 - mae: 0.5672\n",
      "Epoch 24/50\n",
      "1833/1833 [==============================] - 0s 181us/step - loss: 0.5022 - mae: 0.5621\n",
      "Epoch 25/50\n",
      "1833/1833 [==============================] - 0s 181us/step - loss: 0.5010 - mae: 0.5547\n",
      "Epoch 26/50\n",
      "1833/1833 [==============================] - 0s 185us/step - loss: 0.5071 - mae: 0.5601\n",
      "Epoch 27/50\n",
      "1833/1833 [==============================] - 0s 183us/step - loss: 0.4969 - mae: 0.5567\n",
      "Epoch 28/50\n",
      "1833/1833 [==============================] - 0s 185us/step - loss: 0.5011 - mae: 0.5579\n",
      "Epoch 29/50\n",
      "1833/1833 [==============================] - 0s 187us/step - loss: 0.5110 - mae: 0.5600\n",
      "Epoch 30/50\n",
      "1833/1833 [==============================] - 0s 184us/step - loss: 0.4815 - mae: 0.5512\n",
      "Epoch 31/50\n",
      "1833/1833 [==============================] - 0s 184us/step - loss: 0.4927 - mae: 0.5548\n",
      "Epoch 32/50\n",
      "1833/1833 [==============================] - 0s 187us/step - loss: 0.4892 - mae: 0.5520\n",
      "Epoch 33/50\n",
      "1833/1833 [==============================] - 0s 192us/step - loss: 0.4911 - mae: 0.5500\n",
      "Epoch 34/50\n",
      "1833/1833 [==============================] - 0s 203us/step - loss: 0.4913 - mae: 0.5503\n",
      "Epoch 35/50\n",
      "1833/1833 [==============================] - 0s 222us/step - loss: 0.4949 - mae: 0.5516\n",
      "Epoch 36/50\n",
      "1833/1833 [==============================] - 0s 205us/step - loss: 0.4695 - mae: 0.5373\n",
      "Epoch 37/50\n",
      "1833/1833 [==============================] - 0s 189us/step - loss: 0.4741 - mae: 0.5442\n",
      "Epoch 38/50\n",
      "1833/1833 [==============================] - 0s 181us/step - loss: 0.4478 - mae: 0.5240\n",
      "Epoch 39/50\n",
      "1833/1833 [==============================] - 0s 232us/step - loss: 0.4810 - mae: 0.5451\n",
      "Epoch 40/50\n",
      "1833/1833 [==============================] - 0s 193us/step - loss: 0.4736 - mae: 0.5402\n",
      "Epoch 41/50\n",
      "1833/1833 [==============================] - 0s 209us/step - loss: 0.4768 - mae: 0.5465\n",
      "Epoch 42/50\n",
      "1833/1833 [==============================] - 0s 187us/step - loss: 0.4613 - mae: 0.5396\n",
      "Epoch 43/50\n",
      "1833/1833 [==============================] - 0s 188us/step - loss: 0.4577 - mae: 0.5358\n",
      "Epoch 44/50\n",
      "1833/1833 [==============================] - 0s 181us/step - loss: 0.4598 - mae: 0.5348\n",
      "Epoch 45/50\n",
      "1833/1833 [==============================] - 0s 184us/step - loss: 0.4457 - mae: 0.5257\n",
      "Epoch 46/50\n",
      "1833/1833 [==============================] - 0s 188us/step - loss: 0.4642 - mae: 0.5363\n",
      "Epoch 47/50\n",
      "1833/1833 [==============================] - 0s 183us/step - loss: 0.4508 - mae: 0.5304\n",
      "Epoch 48/50\n",
      "1833/1833 [==============================] - 0s 183us/step - loss: 0.4724 - mae: 0.5412\n",
      "Epoch 49/50\n",
      "1833/1833 [==============================] - 0s 207us/step - loss: 0.4432 - mae: 0.5242\n",
      "Epoch 50/50\n",
      "1833/1833 [==============================] - 0s 185us/step - loss: 0.4432 - mae: 0.5247\n",
      "Kappa Score for category 0: 0.7936155395358467\n",
      "Kappa Score for category 1: 0.7641147682260576\n",
      "Kappa Score for category 2: 0.7855319148936171\n",
      "Kappa Score for category 3: 0.6806436687059791\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 814,900\n",
      "Trainable params: 814,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1834/1834 [==============================] - 1s 396us/step - loss: 2.6326 - mae: 1.2851\n",
      "Epoch 2/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.9970 - mae: 0.7872\n",
      "Epoch 3/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.8574 - mae: 0.7267\n",
      "Epoch 4/50\n",
      "1834/1834 [==============================] - 0s 192us/step - loss: 0.7972 - mae: 0.7044\n",
      "Epoch 5/50\n",
      "1834/1834 [==============================] - 0s 183us/step - loss: 0.7652 - mae: 0.6904\n",
      "Epoch 6/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.7039 - mae: 0.6613\n",
      "Epoch 7/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.6736 - mae: 0.6457\n",
      "Epoch 8/50\n",
      "1834/1834 [==============================] - 0s 182us/step - loss: 0.6411 - mae: 0.6309\n",
      "Epoch 9/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.6372 - mae: 0.6281\n",
      "Epoch 10/50\n",
      "1834/1834 [==============================] - 0s 188us/step - loss: 0.6185 - mae: 0.6219\n",
      "Epoch 11/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.5988 - mae: 0.6098\n",
      "Epoch 12/50\n",
      "1834/1834 [==============================] - 0s 189us/step - loss: 0.5940 - mae: 0.6089\n",
      "Epoch 13/50\n",
      "1834/1834 [==============================] - 0s 196us/step - loss: 0.5765 - mae: 0.5935\n",
      "Epoch 14/50\n",
      "1834/1834 [==============================] - 0s 191us/step - loss: 0.5639 - mae: 0.5894\n",
      "Epoch 15/50\n",
      "1834/1834 [==============================] - 0s 187us/step - loss: 0.5632 - mae: 0.5880\n",
      "Epoch 16/50\n",
      "1834/1834 [==============================] - 0s 190us/step - loss: 0.5559 - mae: 0.5872\n",
      "Epoch 17/50\n",
      "1834/1834 [==============================] - 0s 193us/step - loss: 0.5352 - mae: 0.5715\n",
      "Epoch 18/50\n",
      "1834/1834 [==============================] - 0s 187us/step - loss: 0.5368 - mae: 0.5762\n",
      "Epoch 19/50\n",
      "1834/1834 [==============================] - 0s 184us/step - loss: 0.5332 - mae: 0.5724\n",
      "Epoch 20/50\n",
      "1834/1834 [==============================] - 0s 180us/step - loss: 0.5489 - mae: 0.5833\n",
      "Epoch 21/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.5291 - mae: 0.5687\n",
      "Epoch 22/50\n",
      "1834/1834 [==============================] - 0s 187us/step - loss: 0.5121 - mae: 0.5615\n",
      "Epoch 23/50\n",
      "1834/1834 [==============================] - 0s 186us/step - loss: 0.5310 - mae: 0.5675\n",
      "Epoch 24/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.5415 - mae: 0.5747\n",
      "Epoch 25/50\n",
      "1834/1834 [==============================] - 0s 180us/step - loss: 0.5047 - mae: 0.5565\n",
      "Epoch 26/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.5010 - mae: 0.5565\n",
      "Epoch 27/50\n",
      "1834/1834 [==============================] - 0s 188us/step - loss: 0.5011 - mae: 0.5568\n",
      "Epoch 28/50\n",
      "1834/1834 [==============================] - 0s 179us/step - loss: 0.5101 - mae: 0.5595\n",
      "Epoch 29/50\n",
      "1834/1834 [==============================] - 0s 189us/step - loss: 0.4908 - mae: 0.5503\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1834/1834 [==============================] - 0s 199us/step - loss: 0.4805 - mae: 0.5409\n",
      "Epoch 31/50\n",
      "1834/1834 [==============================] - 0s 184us/step - loss: 0.4889 - mae: 0.5524\n",
      "Epoch 32/50\n",
      "1834/1834 [==============================] - 0s 184us/step - loss: 0.4992 - mae: 0.5581\n",
      "Epoch 33/50\n",
      "1834/1834 [==============================] - 0s 184us/step - loss: 0.4807 - mae: 0.5443\n",
      "Epoch 34/50\n",
      "1834/1834 [==============================] - 0s 180us/step - loss: 0.4764 - mae: 0.5416\n",
      "Epoch 35/50\n",
      "1834/1834 [==============================] - 0s 208us/step - loss: 0.4813 - mae: 0.5414\n",
      "Epoch 36/50\n",
      "1834/1834 [==============================] - 0s 192us/step - loss: 0.4823 - mae: 0.5479\n",
      "Epoch 37/50\n",
      "1834/1834 [==============================] - 0s 181us/step - loss: 0.4901 - mae: 0.5500\n",
      "Epoch 38/50\n",
      "1834/1834 [==============================] - 0s 186us/step - loss: 0.4945 - mae: 0.5527\n",
      "Epoch 39/50\n",
      "1834/1834 [==============================] - 0s 194us/step - loss: 0.4722 - mae: 0.5395\n",
      "Epoch 40/50\n",
      "1834/1834 [==============================] - 0s 181us/step - loss: 0.4752 - mae: 0.5412\n",
      "Epoch 41/50\n",
      "1834/1834 [==============================] - 0s 183us/step - loss: 0.4643 - mae: 0.5409\n",
      "Epoch 42/50\n",
      "1834/1834 [==============================] - 0s 182us/step - loss: 0.4625 - mae: 0.5395\n",
      "Epoch 43/50\n",
      "1834/1834 [==============================] - 0s 184us/step - loss: 0.4631 - mae: 0.5306\n",
      "Epoch 44/50\n",
      "1834/1834 [==============================] - 0s 186us/step - loss: 0.4605 - mae: 0.5322\n",
      "Epoch 45/50\n",
      "1834/1834 [==============================] - 0s 182us/step - loss: 0.4627 - mae: 0.5336\n",
      "Epoch 46/50\n",
      "1834/1834 [==============================] - 0s 180us/step - loss: 0.4555 - mae: 0.5259\n",
      "Epoch 47/50\n",
      "1834/1834 [==============================] - 0s 191us/step - loss: 0.4352 - mae: 0.5209\n",
      "Epoch 48/50\n",
      "1834/1834 [==============================] - 0s 187us/step - loss: 0.4487 - mae: 0.5259\n",
      "Epoch 49/50\n",
      "1834/1834 [==============================] - 0s 186us/step - loss: 0.4548 - mae: 0.5290\n",
      "Epoch 50/50\n",
      "1834/1834 [==============================] - 0s 189us/step - loss: 0.4582 - mae: 0.5313\n",
      "Kappa Score for category 0: 0.7739299863609794\n",
      "Kappa Score for category 1: 0.7531046599848261\n",
      "Kappa Score for category 2: 0.7607580382520996\n",
      "Kappa Score for category 3: 0.6518285582906262\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 814,900\n",
      "Trainable params: 814,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1834/1834 [==============================] - 1s 409us/step - loss: 2.8670 - mae: 1.3349\n",
      "Epoch 2/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 1.0976 - mae: 0.8289\n",
      "Epoch 3/50\n",
      "1834/1834 [==============================] - 0s 188us/step - loss: 0.9016 - mae: 0.7411\n",
      "Epoch 4/50\n",
      "1834/1834 [==============================] - 0s 182us/step - loss: 0.8322 - mae: 0.7175\n",
      "Epoch 5/50\n",
      "1834/1834 [==============================] - 0s 184us/step - loss: 0.7761 - mae: 0.6920\n",
      "Epoch 6/50\n",
      "1834/1834 [==============================] - 0s 183us/step - loss: 0.7260 - mae: 0.6698\n",
      "Epoch 7/50\n",
      "1834/1834 [==============================] - 0s 186us/step - loss: 0.6943 - mae: 0.6575\n",
      "Epoch 8/50\n",
      "1834/1834 [==============================] - 0s 187us/step - loss: 0.6495 - mae: 0.6349\n",
      "Epoch 9/50\n",
      "1834/1834 [==============================] - 0s 188us/step - loss: 0.6336 - mae: 0.6234\n",
      "Epoch 10/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.6452 - mae: 0.6260\n",
      "Epoch 11/50\n",
      "1834/1834 [==============================] - 0s 192us/step - loss: 0.6270 - mae: 0.6147\n",
      "Epoch 12/50\n",
      "1834/1834 [==============================] - 0s 193us/step - loss: 0.5955 - mae: 0.6105\n",
      "Epoch 13/50\n",
      "1834/1834 [==============================] - 0s 182us/step - loss: 0.5995 - mae: 0.6076\n",
      "Epoch 14/50\n",
      "1834/1834 [==============================] - 0s 188us/step - loss: 0.5908 - mae: 0.5983\n",
      "Epoch 15/50\n",
      "1834/1834 [==============================] - 0s 191us/step - loss: 0.5834 - mae: 0.6020\n",
      "Epoch 16/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.5814 - mae: 0.5951\n",
      "Epoch 17/50\n",
      "1834/1834 [==============================] - 0s 183us/step - loss: 0.5547 - mae: 0.5814\n",
      "Epoch 18/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.5521 - mae: 0.5869\n",
      "Epoch 19/50\n",
      "1834/1834 [==============================] - 0s 183us/step - loss: 0.5461 - mae: 0.5796\n",
      "Epoch 20/50\n",
      "1834/1834 [==============================] - 0s 183us/step - loss: 0.5371 - mae: 0.5707\n",
      "Epoch 21/50\n",
      "1834/1834 [==============================] - 0s 189us/step - loss: 0.5291 - mae: 0.5657\n",
      "Epoch 22/50\n",
      "1834/1834 [==============================] - 0s 182us/step - loss: 0.5359 - mae: 0.5711\n",
      "Epoch 23/50\n",
      "1834/1834 [==============================] - 0s 188us/step - loss: 0.5473 - mae: 0.5802\n",
      "Epoch 24/50\n",
      "1834/1834 [==============================] - 0s 188us/step - loss: 0.5114 - mae: 0.5586\n",
      "Epoch 25/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.5356 - mae: 0.5712\n",
      "Epoch 26/50\n",
      "1834/1834 [==============================] - 0s 193us/step - loss: 0.5079 - mae: 0.5591\n",
      "Epoch 27/50\n",
      "1834/1834 [==============================] - 0s 186us/step - loss: 0.5140 - mae: 0.5590\n",
      "Epoch 28/50\n",
      "1834/1834 [==============================] - 0s 188us/step - loss: 0.5171 - mae: 0.5627\n",
      "Epoch 29/50\n",
      "1834/1834 [==============================] - 0s 186us/step - loss: 0.5078 - mae: 0.5561\n",
      "Epoch 30/50\n",
      "1834/1834 [==============================] - 0s 189us/step - loss: 0.5087 - mae: 0.5599\n",
      "Epoch 31/50\n",
      "1834/1834 [==============================] - 0s 190us/step - loss: 0.5202 - mae: 0.5643\n",
      "Epoch 32/50\n",
      "1834/1834 [==============================] - 0s 190us/step - loss: 0.5005 - mae: 0.5562\n",
      "Epoch 33/50\n",
      "1834/1834 [==============================] - 0s 186us/step - loss: 0.5097 - mae: 0.5613\n",
      "Epoch 34/50\n",
      "1834/1834 [==============================] - 0s 183us/step - loss: 0.4994 - mae: 0.5477\n",
      "Epoch 35/50\n",
      "1834/1834 [==============================] - 0s 188us/step - loss: 0.4922 - mae: 0.5535\n",
      "Epoch 36/50\n",
      "1834/1834 [==============================] - 0s 184us/step - loss: 0.4956 - mae: 0.5505\n",
      "Epoch 37/50\n",
      "1834/1834 [==============================] - 0s 184us/step - loss: 0.5005 - mae: 0.5518\n",
      "Epoch 38/50\n",
      "1834/1834 [==============================] - 0s 188us/step - loss: 0.4817 - mae: 0.5457\n",
      "Epoch 39/50\n",
      "1834/1834 [==============================] - 0s 191us/step - loss: 0.4806 - mae: 0.5397\n",
      "Epoch 40/50\n",
      "1834/1834 [==============================] - 0s 186us/step - loss: 0.4836 - mae: 0.5439\n",
      "Epoch 41/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.4913 - mae: 0.5483\n",
      "Epoch 42/50\n",
      "1834/1834 [==============================] - 0s 192us/step - loss: 0.4647 - mae: 0.5386\n",
      "Epoch 43/50\n",
      "1834/1834 [==============================] - 0s 192us/step - loss: 0.4820 - mae: 0.5444\n",
      "Epoch 44/50\n",
      "1834/1834 [==============================] - 0s 187us/step - loss: 0.4639 - mae: 0.5305\n",
      "Epoch 45/50\n",
      "1834/1834 [==============================] - 0s 184us/step - loss: 0.4604 - mae: 0.5309\n",
      "Epoch 46/50\n",
      "1834/1834 [==============================] - 0s 188us/step - loss: 0.4634 - mae: 0.5356\n",
      "Epoch 47/50\n",
      "1834/1834 [==============================] - 0s 189us/step - loss: 0.4643 - mae: 0.5361\n",
      "Epoch 48/50\n",
      "1834/1834 [==============================] - 0s 187us/step - loss: 0.4673 - mae: 0.5384\n",
      "Epoch 49/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.4848 - mae: 0.5461\n",
      "Epoch 50/50\n",
      "1834/1834 [==============================] - 0s 184us/step - loss: 0.4294 - mae: 0.5128\n",
      "Kappa Score for category 0: 0.7713497535390529\n",
      "Kappa Score for category 1: 0.7605145939964034\n",
      "Kappa Score for category 2: 0.778270659761315\n",
      "Kappa Score for category 3: 0.6823949480642115\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 814,900\n",
      "Trainable params: 814,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1834/1834 [==============================] - 1s 419us/step - loss: 2.3646 - mae: 1.2043\n",
      "Epoch 2/50\n",
      "1834/1834 [==============================] - 0s 187us/step - loss: 0.9268 - mae: 0.7620\n",
      "Epoch 3/50\n",
      "1834/1834 [==============================] - 0s 182us/step - loss: 0.8432 - mae: 0.7208\n",
      "Epoch 4/50\n",
      "1834/1834 [==============================] - 0s 190us/step - loss: 0.7712 - mae: 0.6932\n",
      "Epoch 5/50\n",
      "1834/1834 [==============================] - 0s 191us/step - loss: 0.7456 - mae: 0.6789\n",
      "Epoch 6/50\n",
      "1834/1834 [==============================] - 0s 181us/step - loss: 0.7148 - mae: 0.6645\n",
      "Epoch 7/50\n",
      "1834/1834 [==============================] - 0s 187us/step - loss: 0.6473 - mae: 0.6311\n",
      "Epoch 8/50\n",
      "1834/1834 [==============================] - 0s 186us/step - loss: 0.6249 - mae: 0.6217\n",
      "Epoch 9/50\n",
      "1834/1834 [==============================] - 0s 189us/step - loss: 0.6197 - mae: 0.6125\n",
      "Epoch 10/50\n",
      "1834/1834 [==============================] - 0s 187us/step - loss: 0.6100 - mae: 0.6145\n",
      "Epoch 11/50\n",
      "1834/1834 [==============================] - 0s 186us/step - loss: 0.6102 - mae: 0.6092\n",
      "Epoch 12/50\n",
      "1834/1834 [==============================] - 0s 186us/step - loss: 0.6056 - mae: 0.6085\n",
      "Epoch 13/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.5732 - mae: 0.5955\n",
      "Epoch 14/50\n",
      "1834/1834 [==============================] - 0s 184us/step - loss: 0.5664 - mae: 0.5857\n",
      "Epoch 15/50\n",
      "1834/1834 [==============================] - 0s 193us/step - loss: 0.5613 - mae: 0.5872\n",
      "Epoch 16/50\n",
      "1834/1834 [==============================] - 0s 191us/step - loss: 0.5355 - mae: 0.5738\n",
      "Epoch 17/50\n",
      "1834/1834 [==============================] - 0s 188us/step - loss: 0.5483 - mae: 0.5760\n",
      "Epoch 18/50\n",
      "1834/1834 [==============================] - 0s 192us/step - loss: 0.5636 - mae: 0.5859\n",
      "Epoch 19/50\n",
      "1834/1834 [==============================] - 0s 188us/step - loss: 0.5129 - mae: 0.5678\n",
      "Epoch 20/50\n",
      "1834/1834 [==============================] - 0s 183us/step - loss: 0.5130 - mae: 0.5571\n",
      "Epoch 21/50\n",
      "1834/1834 [==============================] - 0s 191us/step - loss: 0.5376 - mae: 0.5742\n",
      "Epoch 22/50\n",
      "1834/1834 [==============================] - 0s 187us/step - loss: 0.5213 - mae: 0.5660\n",
      "Epoch 23/50\n",
      "1834/1834 [==============================] - 0s 188us/step - loss: 0.5484 - mae: 0.5820\n",
      "Epoch 24/50\n",
      "1834/1834 [==============================] - 0s 187us/step - loss: 0.5198 - mae: 0.5658\n",
      "Epoch 25/50\n",
      "1834/1834 [==============================] - 0s 186us/step - loss: 0.5174 - mae: 0.5589\n",
      "Epoch 26/50\n",
      "1834/1834 [==============================] - 0s 189us/step - loss: 0.5021 - mae: 0.5541\n",
      "Epoch 27/50\n",
      "1834/1834 [==============================] - 0s 186us/step - loss: 0.4969 - mae: 0.5549\n",
      "Epoch 28/50\n",
      "1834/1834 [==============================] - 0s 183us/step - loss: 0.4984 - mae: 0.5545\n",
      "Epoch 29/50\n",
      "1834/1834 [==============================] - 0s 184us/step - loss: 0.4932 - mae: 0.5523\n",
      "Epoch 30/50\n",
      "1834/1834 [==============================] - 0s 196us/step - loss: 0.4802 - mae: 0.5405\n",
      "Epoch 31/50\n",
      "1834/1834 [==============================] - 0s 188us/step - loss: 0.4890 - mae: 0.5475\n",
      "Epoch 32/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.4885 - mae: 0.5481\n",
      "Epoch 33/50\n",
      "1834/1834 [==============================] - 0s 189us/step - loss: 0.5037 - mae: 0.5522\n",
      "Epoch 34/50\n",
      "1834/1834 [==============================] - 0s 189us/step - loss: 0.4739 - mae: 0.5382\n",
      "Epoch 35/50\n",
      "1834/1834 [==============================] - 0s 179us/step - loss: 0.4922 - mae: 0.5501\n",
      "Epoch 36/50\n",
      "1834/1834 [==============================] - 0s 186us/step - loss: 0.4810 - mae: 0.5465\n",
      "Epoch 37/50\n",
      "1834/1834 [==============================] - 0s 194us/step - loss: 0.4774 - mae: 0.5430\n",
      "Epoch 38/50\n",
      "1834/1834 [==============================] - 0s 182us/step - loss: 0.5037 - mae: 0.5548\n",
      "Epoch 39/50\n",
      "1834/1834 [==============================] - 0s 186us/step - loss: 0.4736 - mae: 0.5397\n",
      "Epoch 40/50\n",
      "1834/1834 [==============================] - 0s 186us/step - loss: 0.4727 - mae: 0.5398\n",
      "Epoch 41/50\n",
      "1834/1834 [==============================] - 0s 189us/step - loss: 0.4673 - mae: 0.5400\n",
      "Epoch 42/50\n",
      "1834/1834 [==============================] - 0s 187us/step - loss: 0.4719 - mae: 0.5399\n",
      "Epoch 43/50\n",
      "1834/1834 [==============================] - 0s 192us/step - loss: 0.4669 - mae: 0.5370\n",
      "Epoch 44/50\n",
      "1834/1834 [==============================] - 0s 181us/step - loss: 0.4672 - mae: 0.5367\n",
      "Epoch 45/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.4591 - mae: 0.5292\n",
      "Epoch 46/50\n",
      "1834/1834 [==============================] - 0s 187us/step - loss: 0.4530 - mae: 0.5258\n",
      "Epoch 47/50\n",
      "1834/1834 [==============================] - 0s 185us/step - loss: 0.4561 - mae: 0.5275\n",
      "Epoch 48/50\n",
      "1834/1834 [==============================] - 0s 188us/step - loss: 0.4663 - mae: 0.5416\n",
      "Epoch 49/50\n",
      "1834/1834 [==============================] - 0s 189us/step - loss: 0.4691 - mae: 0.5316\n",
      "Epoch 50/50\n",
      "1834/1834 [==============================] - 0s 184us/step - loss: 0.4599 - mae: 0.5303\n",
      "Kappa Score for category 0: 0.8106986997125367\n",
      "Kappa Score for category 1: 0.7815573234683155\n",
      "Kappa Score for category 2: 0.7883230252327761\n",
      "Kappa Score for category 3: 0.7174014514530516\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# cv = KFold(len(X), n_folds=5, shuffle=True)\n",
    "kf = KFold(n_splits=5,shuffle=True)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "\n",
    "count = 1\n",
    "for traincv, testcv in kf.split(normed_x):\n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "    X_test, X_train, y_test, y_train = new_x.iloc[testcv], new_x.iloc[traincv], new_y.iloc[testcv], new_y.iloc[traincv]\n",
    "    \n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "    \n",
    "    # making predictions on the validation dataset here:\n",
    "    validation_essays = val_x['essay']\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "            # Obtaining all sentences from the training essays.\n",
    "            sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "            \n",
    "    # new aidan, just add some more sentences to help out the word2vec model\n",
    "    for essay in validation_essays:\n",
    "        sentences += essay\n",
    "            \n",
    "    # Initializing variables for word2vec model.\n",
    "    num_features = 300 \n",
    "    min_word_count = 40\n",
    "    num_workers = 4\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "\n",
    "    print(\"Training Word2Vec Model...\")\n",
    "    # new Aidan\n",
    "#     model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling) \n",
    "    model = Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "#     model.init_sims(replace=True)\n",
    "    model.wv.save_word2vec_format('word2vecmodel_4targets.bin', binary=True)\n",
    "\n",
    "    clean_train_essays = []\n",
    "    \n",
    "    # Generate training and testing data word vectors.\n",
    "    for essay_v in train_essays:\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "    \n",
    "    clean_test_essays = []\n",
    "    \n",
    "    for essay_v in test_essays:\n",
    "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
    "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "    \n",
    "    # Copy the logic for training and testing to make validation data word vectors\n",
    "    clean_val_essays = []\n",
    "    for essay_v in validation_essays:\n",
    "        clean_val_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True))\n",
    "    valDataVecs = getAvgFeatureVecs(clean_val_essays, model, num_features)\n",
    "    \n",
    "    \n",
    "    \n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    valDataVecs = np.array(valDataVecs)\n",
    "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "    valDataVecs = np.reshape(valDataVecs, (valDataVecs.shape[0], 1, valDataVecs.shape[1]))\n",
    "    \n",
    "    lstm_model = get_model()\n",
    "    # originall epochs 50, batch_size 64\n",
    "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
    "    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "    # Save any one of the 8 models.\n",
    "    if count == 5:\n",
    "         lstm_model.save('./model_weights/final_lstm_4targets.h5')\n",
    "    \n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred = np.around(y_pred)\n",
    "    y_test = np.around(y_test)\n",
    "#     print(y_pred[:,1][:25])\n",
    "\n",
    "\n",
    "    # new aidan \n",
    "    y_pred_categories = []\n",
    "    y_test_values_categories = []\n",
    "    result = []\n",
    "    for category_index in range(4):\n",
    "        y_pred_categories.append(y_pred[:,category_index]) # actually unnecessary\n",
    "        y_test_values_categories.append(y_test.values[:,category_index]) # unnecessary\n",
    "        result.append(cohen_kappa_score(y_test.values[:,category_index],y_pred[:,category_index],weights='quadratic'))\n",
    "        print(\"Kappa Score for category {}: {}\".format(category_index, result[-1]))                                                                       \n",
    "    \n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    \n",
    "    # old\n",
    "#     result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "#     print(\"Kappa Score: {}\".format(result))\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12976"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(for_kappa['rater1_domain1'], for_kappa['rater2_domain1'], weights='quadratic')\n",
    "len(for_cappa['rater1_domain1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Avg. Kappa Score is 0.961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Kappa score after a 5-fold cross validation:  0.7546\n",
      "Kappa Scores: Fold by Writing Evaluation metric\n",
      "[[0.8057 0.7818 0.7926 0.6573]\n",
      " [0.7936 0.7641 0.7855 0.6806]\n",
      " [0.7739 0.7531 0.7608 0.6518]\n",
      " [0.7713 0.7605 0.7783 0.6824]\n",
      " [0.8107 0.7816 0.7883 0.7174]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))\n",
    "print(\"Kappa Scores: Fold by Writing Evaluation metric\")\n",
    "print(np.around(np.array(results),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 2. 2. 2.]\n",
      " [1. 2. 2. 2.]\n",
      " [0. 1. 2. 2.]\n",
      " [2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]\n",
      " [2. 2. 1. 0.]\n",
      " [3. 3. 2. 2.]\n",
      " [0. 1. 1. 2.]\n",
      " [3. 3. 3. 3.]\n",
      " [2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]\n",
      " [0. 2. 2. 2.]\n",
      " [1. 1. 2. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [2. 2. 2. 2.]\n",
      " [1. 1. 2. 2.]\n",
      " [2. 2. 2. 2.]\n",
      " [0. 1. 1. 2.]\n",
      " [0. 2. 2. 3.]\n",
      " [2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]\n",
      " [3. 3. 3. 3.]\n",
      " [2. 2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# print(y_pred[:25])\n",
    "# print(len(y_pred[:,1]))\n",
    "# print(type(y_pred))\n",
    "\n",
    "print(y_test.values[:25])\n",
    "# np.around(y_test.values[:,category_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your essay begins: Dear @ORGANIZATION1, @CAPS1 has been brought to my attention that some people feel that computers are bad for us. Some people say that they are a distraction to our physicaland mental health. Although ...\n",
      "\n",
      "After cleaning, the essay still has 254 of its original 488 words\n",
      "For an essay from set 7\n",
      "\n",
      "You scored:\n",
      "Ideas 3.6127977\n",
      "Organization 3.6323524\n",
      "Style 3.6127977\n",
      "Conventions 3.6127977\n",
      "where each score is out of 3\n",
      "in total, you scored a 14.5 out of 60\n",
      "4.3653913\n"
     ]
    }
   ],
   "source": [
    "# Can I test on a sample essay?\n",
    "\n",
    "# Yes, just put the essay in here\n",
    "\n",
    "essay_file = open(os.path.join(DATASET_DIR, 'testEssay.txt'))\n",
    "essay_file = open(os.path.join(DATASET_DIR, 'essay7_1.txt'))\n",
    "essay_file = open(os.path.join(DATASET_DIR, 'testEssay8_bad.txt'))\n",
    "# essay_file = open(os.path.join(DATASET_DIR, 'testEssay8_best.txt'))\n",
    "essay = essay_file.read()\n",
    "bad_essay = \"I don't like computers because they don't work.\"\n",
    "essay = bad_essay\n",
    "essay = \"Dear @ORGANIZATION1, @CAPS1 has been brought to my attention that some people feel that computers are bad for us. Some people say that they are a distraction to our physicaland mental health. Although I can see how some people would think this, I believe that computers are a good benifit to all society. I believe this because computers can help people learn, stay intach with friends or family that live faraway, and stay orginized. Sometimes people are on the computer, learning and they don't even know @CAPS1. Simply by visiting the @ORGANIZATION2 homepage, you automaticly see the news feeds of things happening around the world. Other times people go online diliberatly to learn. If someone is thinking about going to @LOCATION1 then they would probably go on the internet to learn about @CAPS1. Simply by searching equadore many choices will pop up you climate, sesonal weather, hotel options, and other farts. But thats not the only way people are learning on the internet. Now, many college students have the option of taking their lessons online. This is because some students like calm quietness or own house the distractions of sitting in class. Friends could be a big distraction in class, but how can you stay intouch with your friends if they moved away? I remember in second grade my bestfriend, @LOCATION2, move away. I was so sad. I badey ever talked to her, but then one day our parents set us up on a vidio chat! I felt like I was right their with her! This was great, and I though about how many people could use this to talk to relatives or friends. Another great way to stay intouch into friends and family is through e-mail. By writing a message and sending @CAPS1 can make staying in touch so easey, and your personal wants can chat and emails are a easey thing to send world wide. So many people love to type on a keyboard as well, but so many different papers that you type could be lost. I, for me, hate clutter, and I have so many school binders for papers to be lost in. This is why I take great advantage of typing my paper every chance I get. My computer keeps me orginiced because I could never loose my work. File save, is an idiot proof way to keep all your files in a safe place. Then all you have to do is press print to get a hard copy. I am sure that many people love using their computer for the same reason. Also, I myself am a much faster typer than I am writer so my work is a lot needey on the computer. As you can see their are plenty of reasons why using a computer is goof for our society you can learn, stay intouch with friends and family, and stay orginiced. Many people, could agree with me. Don't you?\"\n",
    "print(\"Your essay begins:\", essay[0:200],\"...\")\n",
    "print('')\n",
    "\n",
    "clean_essay = essay_to_wordlist( essay, remove_stopwords=True)\n",
    "print(\"After cleaning, the essay still has {} of its original {} words\".format(len(clean_essay),len(essay.split(' '))))\n",
    "\n",
    "vec = getAvgFeatureVecs([clean_essay], model, num_features)\n",
    "\n",
    "vec = np.array(vec)\n",
    "\n",
    "vec = np.reshape(vec, (vec.shape[0], 1, vec.shape[1]))\n",
    "prediction = lstm_model.predict(vec)[0]\n",
    "rounded_preds = [round(ele,9) for ele in prediction]\n",
    "\n",
    "predictions = lstm_model.predict(valDataVecs)\n",
    "\n",
    "\n",
    "print(\"For an essay from set 7\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"You scored:\")\n",
    "print(\"Ideas\", rounded_preds[0])\n",
    "print(\"Organization\", rounded_preds[1])\n",
    "print(\"Style\", rounded_preds[0])\n",
    "print(\"Conventions\", rounded_preds[0])\n",
    "\n",
    "print(\"where each score is out of 3\")\n",
    "print(\"in total, you scored a\", round(sum(prediction),1),\"out of 60\")\n",
    "\n",
    "print(np.amax(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydir = './data/1-TheAthenianSchool-D4zNusnYjVD2oDCbEmHsjjEqtmu'\n",
    "files = [f for f in listdir(mydir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning, the essay still has 254 of its original 488 words\n",
      "[1.8167508, 2.2781928, 2.4870255, 2.6471348]\n"
     ]
    }
   ],
   "source": [
    "essays = []\n",
    "for f in files:\n",
    "    with pdfplumber.open(mydir+'/'+f) as pdf:\n",
    "        second_page = pdf.pages[1]\n",
    "        sp_text = second_page.extract_text()\n",
    "        ind = sp_text.find(\"why?\")\n",
    "        rind = sp_text.rfind('\\n')\n",
    "        essays.append(sp_text[ind+5:rind])\n",
    "#         print(essays[0])\n",
    "#         if(essays[0][len(essays[0])-37] == '\\n'):\n",
    "#             print('here')\n",
    "#         print(essays[0][len(essays[0])-37])\n",
    "#         break\n",
    "clean_essays = [essay_to_wordlist( essay, remove_stopwords=True) for essay in essays]\n",
    "print(\"After cleaning, the essay still has {} of its original {} words\".format(len(clean_essay),len(essay.split(' '))))\n",
    "\n",
    "vec = getAvgFeatureVecs(clean_essays, model, num_features)\n",
    "\n",
    "vec = np.array(vec)\n",
    "\n",
    "vec = np.reshape(vec, (vec.shape[0], 1, vec.shape[1]))\n",
    "prediction = lstm_model.predict(vec)[0]\n",
    "rounded_preds = [round(ele,9) for ele in prediction]\n",
    "\n",
    "predictions = lstm_model.predict(valDataVecs)\n",
    "print(rounded_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.8167508  2.2781928  2.4870255  2.6471348 ]\n",
      " [2.345035   2.551067   2.5850112  2.711569  ]\n",
      " [1.7710309  2.206832   2.3629813  2.6072218 ]\n",
      " [3.2058225  3.2856174  3.3571124  3.29809   ]\n",
      " [3.171259   3.2774608  3.38115    3.2882547 ]\n",
      " [3.000753   3.125727   3.1982229  3.1684284 ]\n",
      " [2.101021   2.38502    2.4528663  2.647875  ]\n",
      " [2.6804385  2.8274243  2.8532774  2.9129844 ]\n",
      " [1.8326237  2.2644966  2.4183059  2.5906687 ]\n",
      " [2.2341075  2.5693762  2.6915126  2.8571916 ]\n",
      " [3.4882429  3.5178432  3.5912542  3.4949198 ]\n",
      " [1.8768845  2.2615688  2.3709939  2.550043  ]\n",
      " [3.42253    3.4205554  3.4461586  3.3438635 ]\n",
      " [2.8679528  2.9724307  2.9867928  3.0402021 ]\n",
      " [3.204533   3.2733607  3.325777   3.2580113 ]\n",
      " [3.516319   3.5355601  3.5815074  3.488862  ]\n",
      " [3.2609606  3.3052638  3.3414323  3.283337  ]\n",
      " [3.243958   3.3377683  3.430121   3.355828  ]\n",
      " [0.87444085 1.4784768  1.6871785  1.9964046 ]\n",
      " [3.8672047  3.83946    3.908275   3.7327275 ]\n",
      " [3.396302   3.3703408  3.3769217  3.270807  ]\n",
      " [2.9146132  3.1005864  3.2020414  3.2043138 ]\n",
      " [2.763546   2.9493978  3.0312388  3.0755439 ]\n",
      " [3.4066455  3.4430683  3.499655   3.4101844 ]\n",
      " [3.89335    3.8445385  3.9031875  3.706016  ]\n",
      " [3.2342315  3.3108408  3.3717444  3.32273   ]\n",
      " [3.6098566  3.6202667  3.683801   3.5530562 ]\n",
      " [3.3721323  3.425891   3.4924095  3.4118547 ]\n",
      " [3.8608947  3.826494   3.8902924  3.712885  ]\n",
      " [3.1339488  3.2143757  3.2725127  3.2455492 ]\n",
      " [3.543748   3.5334415  3.584182   3.4284902 ]\n",
      " [2.7458553  2.8726687  2.9068434  2.957199  ]\n",
      " [1.5549226  2.0719843  2.2581112  2.5071826 ]\n",
      " [3.573165   3.5505133  3.6034806  3.4318986 ]\n",
      " [2.724728   2.9240844  3.0135884  3.0748017 ]\n",
      " [3.1750484  3.2768981  3.3680525  3.302549  ]\n",
      " [2.0311713  2.4188082  2.5723605  2.7413511 ]\n",
      " [2.1853726  2.5174928  2.628712   2.8009691 ]\n",
      " [3.361932   3.4515586  3.5157382  3.4396482 ]]\n"
     ]
    }
   ],
   "source": [
    "predictions = lstm_model.predict(vec)\n",
    "print(predictions)\n",
    "# rounded_preds = [round(ele,9) for ele in predictions]\n",
    "\n",
    "# predictions = lstm_model.predict(valDataVecs)\n",
    "# print(rounded_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideas = [prediction[0] for prediction in predictions]\n",
    "organization = [prediction[1] for prediction in predictions]\n",
    "style = [prediction[2] for prediction in predictions]\n",
    "conventions = [prediction[3] for prediction in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f936c7ed9b0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(conventions, kind=\"kde\", bw_adjust=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
